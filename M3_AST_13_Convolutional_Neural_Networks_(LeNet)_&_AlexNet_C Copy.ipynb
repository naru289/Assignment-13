{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-13/blob/main/M3_AST_13_Convolutional_Neural_Networks_(LeNet)_%26_AlexNet_C%20Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 13 : Implementation of Convolutional Neural Networks and how to use Pre-trained (Alexnet) Model using Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "   \n",
        "  At the end of the experiment, you will be able to :\n",
        "    \n",
        "  * understand convolutional neural networks\n",
        "  * understand terms like filters, convolution, pooling, stride\n",
        "  * build CNN model for mnist dataset using pytorch \n",
        "  * understand pre-trained CNN architecture: AlexNet "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hCJasxI__be"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzWueA91AA7q"
      },
      "source": [
        "### Description\n",
        "\n",
        "\n",
        "1. The dataset contains 60,000 Handwritten digits as training samples and 10,000 Test samples, \n",
        "which means each digit occurs 6000 times in the training set and 1000 times in the testing set. \n",
        "2. Each image is Size Normalized and Centered \n",
        "3. Each image is 28 X 28 Pixel with 0-255 Gray Scale Value. \n",
        "4. That means each image is represented as 784 (28 X28) dimension vector where each value is in the range 0- 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Eb8UcIE4r2"
      },
      "source": [
        "### History\n",
        "\n",
        "Yann LeCun (Director of AI Research, Facebook, Courant Institute, NYU) was given the task of identifying the cheque numbers (in the 90’s) and the amount associated with that cheque without manual intervention. That is when this dataset was created which raised the bars and became a benchmark.\n",
        "\n",
        "Yann LeCun and Corinna Cortes (Google Labs, New York) hold the copyright of MNIST dataset, which is a subset of the original NIST datasets. This dataset is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license. \n",
        "\n",
        "It is the handwritten digits dataset in which half of them are written by the Census Bureau employees and remaining by the high school students. The digits collected among the Census Bureau employees are easier and cleaner to recognize than the digits collected among the students.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqAHNnh3E7C0"
      },
      "source": [
        "### Challenges\n",
        "\n",
        "Now, if you notice the images below, you will find that between 2 characters there are always certain similarities and differences. To teach a machine to recognize these patterns and identify the correct output.\n",
        "\n",
        "![altxt](https://www.researchgate.net/profile/Radu_Tudor_Ionescu/publication/282924675/figure/fig3/AS:319968869666820@1453297931093/A-random-sample-of-6-handwritten-digits-from-the-MNIST-data-set-before-and-after.png)\n",
        "\n",
        "Hence, all these challenges make this a good problem to solve in Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9O2_IcPiNYV"
      },
      "source": [
        "## Domain Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X-e2l14iXvo"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Handwriting changes person to person. Some of us have neat handwriting and some have illegible handwriting such as doctors. However, if you think about it even a child who recognizes alphabets and numerics can identify the characters of a text even written by a stranger. But even a technically knowledgeable adult cannot describe the process by which he or she recognizes the text/letters. As you know this is an excellent challenge for Machine Learning.\n",
        "\n",
        "![altxt](https://i.pinimg.com/originals/f2/7a/ac/f27aac4542c0090872110836d65f4c99.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og1EHpbjgrHP"
      },
      "source": [
        "### Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSwgCEA_uJ40"
      },
      "source": [
        "Here, the task is to build a Convolutional Neural Network and pretrained AlexNet model using pytorch that classify the images into the respective handwritten digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OiFi8nj77AW"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2237180\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"6366871391\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GXbNUL2L6LoU"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M3_AST_13_Convolutional_Neural_Networks_(LeNet)_&_AlexNet_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bikcf-BRC24z"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no90mbVqbGJb"
      },
      "source": [
        "* First, we import pytorch, the deep learning library which we’ll be using, and torchvision, which provides our dataset and data transformations. \n",
        "\n",
        "* We also import torch.nn (pytorch’s neural network library), torch.nn.functional (includes non-linear functions like ReLu and sigmoid) and torch.optim for implementing various optimization algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GuIxHsqC240"
      },
      "source": [
        "# Importing Pytorch library\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# Matplotlib is used for ploting graphs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUarXxbEC246"
      },
      "source": [
        "### Loading the MNIST data\n",
        "\n",
        "Now, we'll load the MNIST data. For the first time, we may have to download the data, which can take a while.\n",
        "\n",
        "Now, \n",
        "\n",
        "* We will load both the training set and the testing sets \n",
        "\n",
        "* We will use transform.compose(), which combines all the transformation that are provided to be applied on the dataset. \n",
        "\n",
        "* We will use transforms.ToTensor() which converts the input images to PyTorch tensor.  \n",
        "\n",
        "* We also use transforms.Normalize() which is to scale the input images and the precomputed values (mean and std) for the dataset is passed to the Normalize() method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWvuH-yKDv_1"
      },
      "source": [
        "# Normalize with mean and std (0.1307 and 0.3081 are the mean and std of MNIST data)\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENEeWQzZD0x2"
      },
      "source": [
        "# Loading the train set file\n",
        "mnist_train = datasets.MNIST(root='../data', \n",
        "                            train=True, \n",
        "                            transform=transform,  \n",
        "                            download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSAs4udH_Ugd"
      },
      "source": [
        "# Verifying mean and std of Fashion MNIST data\n",
        "mnist_train.data.float().mean() / 255, mnist_train.data.float().std() / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssNtVBHvFOgL"
      },
      "source": [
        "# Loading the test set file\n",
        "mnist_test = datasets.MNIST(root='../data', \n",
        "                           train=False, \n",
        "                           transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmxkoOr_v6oh"
      },
      "source": [
        " Let’s visualize a image from the training set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywedmu7Rv0dJ"
      },
      "source": [
        "# Plotting one example\n",
        "print(\"Shape of the training data (no of images, height, width) : \", mnist_train.train_data.size()) # (60000, 28, 28)\n",
        "print(\"Shape of the testing data (no of images, height, width) : \", mnist_test.test_data.size())  # (10000, 28, 28)  \n",
        "print(\"\\n\")\n",
        "print(\"An Example Image\")\n",
        "plt.imshow(mnist_train.train_data[0].numpy(), cmap='gray')\n",
        "plt.title('Label : %i' % mnist_train.train_labels[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asuxbx_aBsp5"
      },
      "source": [
        "**torch.utils.data.DataLoader** class represents a Python iterable over a dataset, with following features. \n",
        "\n",
        "1. Batching the data - `batch_size`, which denotes the number of samples (images) contained in each generated batch. The Machine learning dataset can be really large. Hence we cannot often load the entire data into the memory. Hence neural network training is done by loading small batches (commonly called minibatch) of data and using it to update the learnable parameters (weights and biases) of the model.\n",
        "\n",
        "2. Shuffling the data - If set to `shuffle=True`, we will get a new order of exploration at each pass. Shuffling the order in which examples are fed to the classifier is helpful so that batches between epochs do not look alike. By performing it will eventually make our model more robust.\n",
        "\n",
        "\n",
        "\n",
        "The train and test data are provided via data loaders that provide iterators over the datasets to train our models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NDlkt8UGQGX"
      },
      "source": [
        "# The mini batch size used for training\n",
        "batch_size = 100 \n",
        "\n",
        "# Loading the train dataset\n",
        "# Data Loader loads the images and corresponding labels of defined mini batch size.\n",
        "# the image batch shape will be (batch_size, 1, 28, 28)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=mnist_train, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "# Loading the test dataset\n",
        "# Data loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.\n",
        "test_loader = torch.utils.data.DataLoader(dataset=mnist_test, \n",
        "                                          batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06iKhdT5H9tI"
      },
      "source": [
        "Let’s visualize a few images in the mini batch of training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfXCm8nTC25F"
      },
      "source": [
        "batch_count = 0\n",
        "for mini_batch in train_loader:\n",
        "    images, labels = mini_batch    \n",
        "    print('Mini batch size: images -', images.size(), ' labels - ', labels.size())\n",
        "    for j in range(5):  # Basically iterating a few times (hence range(5)) to print a few images in this mini-batch\n",
        "        print(images[j].size(), labels[j])\n",
        "        plt.imshow(images[j][0].numpy(), cmap='gray')\n",
        "        plt.title('Label : %i' % labels[j])\n",
        "        plt.show()\n",
        "\n",
        "        # To plot only 2 images from each batch, applied logic to break out of the loop at range = 1.\n",
        "        if j == 1:\n",
        "            break\n",
        "            \n",
        "    # If you want to visualize images in the next mini-batches you can increase the Batch count value.\n",
        "    if batch_count == 1:\n",
        "        break\n",
        "\n",
        "    batch_count +=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlO2MUP6CaRF"
      },
      "source": [
        "### **What is a Convolutional Neural Network?**\n",
        "\n",
        "CNNs are a subset of the field of computer vision, which is all about applying computational techniques to visual content.\n",
        "\n",
        "People often refer to a CNN as a type of algorithm but it’s actually a combination of different algorithms that work well together.CNNs have two major parts:\n",
        "\n",
        "**1. Feature engineering / preprocessing** – turn our images into set of features that the algorithm can more efficiently interpret.\n",
        "\n",
        "**2. Classification** – train the algorithm to map our images to the given classes and understand the underlying relationship\n",
        "\n",
        "Preprocessing in CNNs is aimed at turning your input images into a set of features that is more informative to the neural net. \n",
        "\n",
        "#### **What is Convolution?**\n",
        "\n",
        "The CNN gets its name from the process of Convolution, which is the first filter applied as part of the feature-engineering step. Think of convolution as applying a filter to our image. We pass over a mini image, usually called a kernel, and output the resulting, filtered subset of our image.\n",
        "\n",
        "\n",
        "For instance, if the input image and the filter look like the following:\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/826/1*4yv0yIH0nVhSOv3AkLUIiw.png\" width=400px/>\n",
        "</center>\n",
        "\n",
        "The filter slides over the input image (green) one pixel at a time starting from the top left. The filter multiplies its own values with the overlapping values of the image while sliding over it and adds all of them up to output a single value for each overlap until the entire image is traversed:\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/536/1*MrGSULUtkXc0Ou07QouV8A.gif\" width=300px/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feZFBVDmGWLu"
      },
      "source": [
        "#### Using pytorch implementation, we will now understand convolutional operations on images\n",
        "\n",
        "* Applying a convolutional filter\n",
        "* Feature Map generation\n",
        "* Stride and Padding\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysxxzh6RGWLv"
      },
      "source": [
        "In Pytorch, \n",
        "\n",
        "* each input image is typically represented as a 3D tensor of shape\n",
        "[channels, height, width]. \n",
        "\n",
        "* A mini-batch is represented as a 4D tensor of shape [minibatch size, channels, height, width]. \n",
        "\n",
        "* The weights of a convolutional layer are represented as a 4D tensor of shape [$f_h$, $f_w$, $f_{n′}$, $f_n$]. \n",
        "\n",
        "* The bias terms of a convolutional layer are simply represented as a 1D tensor of shape [$f_n$].\n",
        "\n",
        "where \n",
        "\n",
        "$f_h$ is filter (or receptive field) height, \n",
        "\n",
        "$f_w$ is filter width, \n",
        "\n",
        "$f_{n′}$ is number of feature maps in the previous layer (layer $l – 1$), and \n",
        "\n",
        "$f_n$ is number of feature maps in the current layer (layer $l$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w98BkTNdKvVs"
      },
      "source": [
        "So, what is a CNN layer? Each convolutional layer has a number of filters, also commonly referred to as kernels. A filter is a (usually) square matrix that slides across the pixels in an image from left-to-right, top-to-bottom. At each \"step\", the filter performs a convolution operation on the image. The output of the convolutional layer is the result of these convolutions after the filter's final \"step\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZzKPBavMFQf"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Filter.png\" width=700px/>\n",
        "</center>\n",
        "$\\hspace{6cm} \\text {Example of a single 2x2 filter passing over an image with image of size 10x10 pixels}$\n",
        "<br><br>\n",
        "\n",
        "The filter (red) slides over the pixels of the image, stepping one pixel at a time. The size of the steps is called the stride, and here we use a stride of one, which means the filter moves one pixel at a time horizontally and moves one pixel down once it reaches the end of a row. The result of the convolution operation (green) is a pixel in the filtered image. All of these convolutions produce a new, filtered image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWXAHGsfQA_d"
      },
      "source": [
        "Notice how the image coming out of the CNN layer is smaller than the image coming into the CNN. This is because the 2x2 filter has only nine steps horizontally and vertically. If we wanted to keep the output image the same size as the input image we could add padding - usually black pixels - around our image. If **zero padding = 1**, there will be one pixel thick around the original image with pixel value = 0. In the below figure we pad a  3×3  input, increasing its size to  5×5 . The corresponding output then increases to a  4×4  matrix. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation:  0×0 + 0×1+ 0×2 + 0×3=0 .\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Padding.png\" width=500px/>\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-h2R0REQKSr"
      },
      "source": [
        "When we have no padding and a stride of one, the output size of Convolution Layer is calculated using the below formula:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$O = \\frac{W- F + 2P}{S} +1$\n",
        "\n",
        "*   O : output height/length\n",
        "*   W : input height/length\n",
        "*   F : filter size (kernel size)\n",
        "*   P : padding - we can add layers of 0s to the outside of the image in order to make sure that the kernel properly passes over the edges of the image. Padding preserves the size of the original image.\n",
        "*   S : stride - the rate at which the kernel passes over the input image. A stride of 2 moves the kernel in 2-pixel increments.\n",
        "\n",
        "\n",
        "The output of the convolution process is called the “convolved feature” or “feature map.”\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dQRzhY8ReYh"
      },
      "source": [
        "Filters applied across an image can be used to detect patterns such as horizontal and vertical lines within an image. These patterns can be thought of as features of the image, which our CNN extracts. These extracted features can then be combined in further layers of the neural network with other extracted features and together create higher level features, e.g. a certain position and orientation of two lines make a cross, which can indicate the center of a handwritten digit '4'.\n",
        "\n",
        "\n",
        "CNNs are also inspired by classic computer vision techniques, like **Sobel filters**. Let's try manually choosing weights of a 3x3 filter to make **Sobel filters** and apply them to some of the MNIST digits to see what type of patterns our CNN layers can learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-F9ETDhR4oX"
      },
      "source": [
        "Apply the filters to the MNIST test images using  `F.conv2d()` or `nn.Conv2d()` function, which is part of pytorch neural network module\n",
        "\n",
        "\n",
        "Here, we use **zero padding** and a **stride** of 1.\n",
        "\n",
        "The plot_filter function takes in a batch of images and a two-dimensional filter and plots the output of that filter applied to all of the images\n",
        "\n",
        "In `F.conv2d()`:\n",
        "\n",
        "*   images is the input mini-batch (a 4D tensor).\n",
        "\n",
        "*   filters is the set of filters to apply (also a 4D tensor).\n",
        "\n",
        "*   stride is equal to 1, but it could also be a 1D array with four elements, where the two central elements are the vertical and horizontal strides ( 𝑠ℎ  and  𝑠𝑤 ). The first and last elements must currently be equal to 1. They may one day be used to specify a batch stride (to skip some instances) and a channel stride (to skip some of the previous layer’s feature maps or channels).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVqDUH1b4Sec"
      },
      "source": [
        "# Defining function to visualize the vertical and horizontal filter\n",
        "def plot_filter(images, filter):\n",
        "\n",
        "    # torch.cat - 'Concatenates' a sequence of tensors along an existing dimension\n",
        "    images = torch.cat([i.unsqueeze(0) for i in images], dim = 0)\n",
        "    filter = torch.FloatTensor(filter).unsqueeze(0).unsqueeze(0)\n",
        "    \n",
        "    # No of images\n",
        "    n_images = images.shape[0]\n",
        "    \n",
        "    # Convolutional 2D - passing images and the filters (horizontal and vertical)\n",
        "    filtered_images = F.conv2d(images, filter, stride=1)\n",
        "\n",
        "    fig = plt.figure(figsize = (20, 5))\n",
        "    \n",
        "    for i in range(n_images):\n",
        "\n",
        "        # Plotting Original Image\n",
        "        ax = fig.add_subplot(2, n_images, i+1)\n",
        "        ax.imshow(images[i].squeeze(0), cmap='gray')\n",
        "        ax.set_title('Original')\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Plotting images after applying filters\n",
        "        image = filtered_images[i].squeeze(0)\n",
        "        ax = fig.add_subplot(2, n_images, n_images+i+1)\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.set_title(f'Filtered')\n",
        "        ax.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Eb6ZrrM4V72"
      },
      "source": [
        "# Visualizing 5 images from the test dataset\n",
        "N_IMAGES = 5\n",
        "\n",
        "images = [image for image, label in [mnist_test[i] for i in range(N_IMAGES)]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lUpEn_55OUs"
      },
      "source": [
        "The first filter is for detecting horizontal lines.\n",
        "\n",
        "We can see on the filtered images that the highest values (the whitest pixels) of the filtered image are where there is a horizontal line that is black on top and white below, e.g. the top of the 7 digit. The lowest values (the blackest pixels) of the filtered image are where there is a horizontal line that goes from white to black, e.g. the bottoms of all of the digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBICGnTe5XNf"
      },
      "source": [
        "horizontal_filter = [[-1, -1, -1],\n",
        "                     [ 0,  0,  0],\n",
        "                     [ 1,  1,  1]]                   \n",
        "\n",
        "plot_filter(images, horizontal_filter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRzABCwY5rwW"
      },
      "source": [
        "By swapping the first and last rows of the above filter, we get a filter that detects horizontal lines from white on top to black underneath.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKd_Rmay5vcS"
      },
      "source": [
        "horizontal_filter = [[ 1,  1,  1],\n",
        "                     [ 0,  0,  0],\n",
        "                     [-1, -1, -1]]\n",
        "\n",
        "plot_filter(images, horizontal_filter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYaIiWeK5_LH"
      },
      "source": [
        "We can also design filters that detect vertical lines.\n",
        "\n",
        "Here's one that detects vertical lines that are black on the left and white on the right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMM2X0Um6AaS"
      },
      "source": [
        "vertical_filter = [[-1, 0, 1],\n",
        "                   [-1, 0, 1],\n",
        "                   [-1, 0, 1]]\n",
        "\n",
        "plot_filter(images, vertical_filter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKlk_r7x6MCP"
      },
      "source": [
        "To get the opposite filter, one that detects vertical lines that are white on the left and black on the right, we swap the left and right columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWpxiUib6NGZ"
      },
      "source": [
        "vertical_filter = [[1, 0, -1],\n",
        "                   [1, 0, -1],\n",
        "                   [1, 0, -1]]\n",
        "\n",
        "plot_filter(images, vertical_filter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50Q0D0DO6YMt"
      },
      "source": [
        "Finally, we'll design a diagonal detecting filter. This one detects lines pointing towards the top right of the image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liHJ6uiJ6ZSy"
      },
      "source": [
        "diagonal_filter = [[-1, -1, 0],\n",
        "                   [-1,  0, 1],\n",
        "                   [ 0,  1, 1]]\n",
        "\n",
        "plot_filter(images, diagonal_filter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZh1eecQGWLy"
      },
      "source": [
        "In the above example, we manually defined the filters, but in a real CNN we would normally define filters as trainable variables so the neural net can learn which filters work best. Instead of manually creating the variables, use the `nn.Conv2D` layer. \n",
        "\n",
        "Also, CNNs' convolutional layers require a huge amount of RAM. This problem is taken care of by the second common building block of CNNs: the pooling layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O6xQA7V-O7s"
      },
      "source": [
        "#### Pooling Layers\n",
        "\n",
        "\n",
        "The goal of the pooling layer is to subsample (i.e., shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters. Each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a receptive field. \n",
        "\n",
        "However, **a pooling neuron has no weights**; all it does is aggregate the inputs using an aggregation function such as the **max** or **mean**. \n",
        "\n",
        "There are two types of widely used pooling in CNN layer:\n",
        "\n",
        "**Max pooling** Max pooling is simply a rule to take the maximum of a region and it helps to proceed with the most important features from the image. Max pooling selects the brighter pixels from the image. It is useful when the background of the image is dark and we are interested in only the lighter pixels of the image. \n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Max_Pooling.png\" width=500px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "**Mean/average** pooling returns the mean/average of the values covered by the filter and we can think of it as equally weighting all features under the filter.\n",
        "\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Average_Pooling.png\" width=500px/>\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hwbLOszr1jA"
      },
      "source": [
        "#### Using pytorch implementation, we will now understand pooling operation on images\n",
        "\n",
        "Let's create a function that allows us to see the outputs of a pooling layer on a batch of images.\n",
        "\n",
        "In `F.conv2d()`:\n",
        "\n",
        "\n",
        "\n",
        "*   images is the input mini-batch (a 4D tensor).\n",
        "*   Kernel_size is the size of the pooling filter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTzkCOwt-QiG"
      },
      "source": [
        "# Define a function to apply pooling layer on the images\n",
        "def plot_subsample(images, pool_type, pool_size):\n",
        "\n",
        "    images = torch.cat([i.unsqueeze(0) for i in images], dim = 0)\n",
        "    \n",
        "    # Check for the pooling type (Max pooling and Average pooling)\n",
        "    if pool_type.lower() == 'max':\n",
        "        pool = F.max_pool2d\n",
        "    elif pool_type.lower() in ['mean', 'avg']:\n",
        "        pool = F.avg_pool2d\n",
        "    else:\n",
        "        raise ValueError(f'pool_type must be either max or mean, got: {pool_type}')\n",
        "    \n",
        "    n_images = images.shape[0]\n",
        "\n",
        "    # Applting Max and Average pooling on the images\n",
        "    pooled_images = pool(images, kernel_size = pool_size)\n",
        "\n",
        "    fig = plt.figure(figsize = (20, 5))\n",
        "    \n",
        "    for i in range(n_images):\n",
        "        # Plot the original image\n",
        "        ax = fig.add_subplot(2, n_images, i+1)\n",
        "        ax.imshow(images[i].squeeze(0), cmap='gray')\n",
        "        ax.set_title('Original')\n",
        "        ax.axis('off')\n",
        "\n",
        "        image = pooled_images[i].squeeze(0)\n",
        "        # Plot the image after after applying the pooling\n",
        "        ax = fig.add_subplot(2, n_images, n_images+i+1)\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.set_title(f'Subsampled')\n",
        "        ax.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8A9hkEq-dlz"
      },
      "source": [
        "The following code creates a max pooling layer using a 2 × 2 kernel and applies it to `images`. The strides default to the kernel size, so this layer will use a stride of 2 (both horizontally and vertically).\n",
        "\n",
        "We can see that the images are heavily downsampled - reduced in size/resolution and in quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vj--57g-evV"
      },
      "source": [
        "plot_subsample(images, 'max', 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLZ6ReA2-lid"
      },
      "source": [
        "If we increase the size of the max pooling filter the images get smaller and the quality is decreased."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ2WU97O-srF"
      },
      "source": [
        "plot_subsample(images, 'max', 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8aYrO1awfiV"
      },
      "source": [
        "### Defining Convoutional Neural Network Architecture (LeNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC1rDpB5wj58"
      },
      "source": [
        "Now we will define a CNN based neural network, that takes the input as 28x28 MNIST images and predicts a label from 0 to 9. The predictions will be of the form of a probability distribution given as an array $P$ of length 10, where each entry $P_i$  denotes the probability of the input image being the digit $i$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6SK6W0MyyBb"
      },
      "source": [
        "**LeNet** is a big breakthrough in the world of image recognition. It is one of the oldest convolution neural networks that was introduced by Yann LeCunn back in 1995 in his research paper. During those days he came up with this LeNet Model to find the handwritten digits representing the Zip codes of the US postal service.\n",
        "\n",
        "The image below shows the architecture of LeNet-5. It consists of two convolutional layers, each followed by a subsampling layer, and then three fully connected linear layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epKavSwZwvFs"
      },
      "source": [
        "The Deep CNN we will be using is called LeNet. A pictorial representation is given below: </br>\n",
        "**NOTE: In the below diagram we're not going to use the Gaussian connections at the end, instead we'll just use a standard linear layer.**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/LeNet.png\" width=900px/>\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIgeu0AqC25L"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "As you can see, the neural network has multiple operations happening one after another. Each operation has learnable parameters (weights and biases). Typically we call them the layers of a neural network. Neural networks can have many layers, and are hence called Deep Neural Networks (DNNs) or Deep CNNs. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6MgNN46B3sE"
      },
      "source": [
        "**Convolutional Layer:**  Convolutional layers are the layers where filters are applied to the original image. The most common type of convolution used is the 2D convolution layer and is abbreviated as **conv2d**. \n",
        "\n",
        "The **kernel** is a filter used to extract the features from the images. The kernel is a matrix that moves over the input data, performs the dot product with the sub-region of input data, and gets the output as the matrix of dot products. \n",
        "\n",
        "Calculate the output size of Convolution Layer, using below formula:\n",
        "\n",
        "  $O = \\frac{W- F + 2P}{S} +1$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The **Pooling layer** is another building block of a CNN. It is used between the convolution layers. One such type of widely used pooling in CNN layer is **Max Pooling**. \n",
        "\n",
        "\n",
        "Output formula for Pooling, $O = \\frac{W- F}{S} +1$   \n",
        "\n",
        "If using PyTorch default stride (default stride is same as kernel size), then output formula for pooling will result,  $O = \\frac{W}{F}$\n",
        "\n",
        "\n",
        "The **Fully connected layers** involves weights, biases, and neurons. It connects neurons in one layer to neurons in another layer. It is used to classify images between different category by training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqycwcVSvhD0"
      },
      "source": [
        "#### Output Size Calculation\n",
        "\n",
        "After Convolutional layer 1 the output size = $\\frac{28-5+ 2(0)}{1}+1 = 24$ ----> Multiply it with 6 filters, the output size becomes (100, 6, 24, 24), 100 ---> mini-batch size\n",
        "\n",
        "Output after appyling Max pool layer of size 2x2 on Convolutional layer 1 = $\\frac{24}{2} = 12$ -----> The output size changes from (100, 6, 24, 24) to (100, 6, 12, 12)\n",
        "\n",
        "After Convolutional layer 2 the output size = $\\frac{12-5+ 2(0)}{1}+1 = 8$ Multiply it with the 16 filters, the output size becomes (100, 16, 8, 8)\n",
        "\n",
        "Output after appyling Maxpool layer of size 2x2 on Convolutional layer 2 = $\\frac{8}{2} = 4$ -----> The output size changes from (100, 16, 8, 8) to (100, 16, 4, 4)\n",
        "\n",
        "The above output (100, 16, 4, 4) is then passed as input to the first fully connected layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I3Af5BzPub2"
      },
      "source": [
        "# A CNN based Feature extractor\n",
        "# Defining neural network in python by a class that inherits from nn.Module\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        \n",
        "        super(LeNet, self).__init__()\n",
        "\n",
        "        # 1st conv layer\n",
        "        # Conv which convolves input image with 6 filters of 5x5 (heightxwidht) size, without padding (padding=0)\n",
        "        self.conv1 = nn.Conv2d(in_channels = 1, \n",
        "                               out_channels = 6, \n",
        "                               kernel_size = (5, 5), \n",
        "                               stride=1)\n",
        "        \n",
        "        # 2nd conv layer\n",
        "        # Conv which convolves input image with 16 filters of 5x5 size, without padding       \n",
        "        self.conv2 = nn.Conv2d(in_channels = 6, \n",
        "                               out_channels = 16, \n",
        "                               kernel_size = (5, 5),\n",
        "                               stride=1)\n",
        "        \n",
        "        # Define the Fully connected layers  \n",
        "        # Linear layer with 120 hidden nodes, taking a flattened [16 x 4 x 4] as input     \n",
        "        self.fc_1 = nn.Linear(16 * 4 * 4, 120)\n",
        "\n",
        "        # Linear layer with 84 hidden nodes\n",
        "        self.fc_2 = nn.Linear(120, 84)\n",
        "        \n",
        "        # Output layer with 10 classes\n",
        "        self.fc_3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # input [1 x 28 x 28]\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        \n",
        "        # output [100 x 6 x 24 x`24]\n",
        "        # 100 ---> batch size\n",
        "        \n",
        "        # Max pooling subsampling operation\n",
        "        x = F.max_pool2d(x, kernel_size = 2)\n",
        "        \n",
        "        # output after applying max pool [100, 6, 12, 12]\n",
        "        \n",
        "        # Non linear activation function\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # input [100 x 6 x 12 x 12]\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        # output [100 x 16 x 8 x 8]\n",
        "        \n",
        "        # Max pooling subsampling operation\n",
        "        x = F.max_pool2d(x, kernel_size = 2)\n",
        "        \n",
        "        # output after applying max pool [100 x 16 x 4 x 4]\n",
        "        \n",
        "        # Non linear activation function\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        # Reshape the 2D to a vector (flatten the image pixels)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        \n",
        "        # x = [batch size, 16*4*4 = 256]\n",
        "                \n",
        "        x = self.fc_1(x)\n",
        "        \n",
        "        # x = [batch size, 120]\n",
        "        \n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        # x = batch size, 84]\n",
        "        \n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc_3(x)\n",
        "        \n",
        "        return x   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFmW4izimMaM"
      },
      "source": [
        "Every Tensor in PyTorch has a **to()** member function. Its job is to put the tensor on which it's called to a certain device whether it be the CPU or a certain GPU.\n",
        "\n",
        "Input to the to function is a torch.device object which can be initialized with either of the following inputs. \n",
        "* cpu for CPU \n",
        "* cuda:0 for putting it on GPU number 0. Similarly, if your system has multiple GPUs, then the respective number would be considered while initializing the device.\n",
        "\n",
        "Generally, whenever you initialize a Tensor, it’s put on the CPU. You should move it to the GPU to make the related calculation faster.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeuqAK-Il3yj"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ElGuL0rLgSj"
      },
      "source": [
        "### Creating an instance of the network\n",
        "Let us declare an object of class LeNet, and make it a CUDA model if CUDA is available:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTCduCI1cusJ"
      },
      "source": [
        "Next we will inspect our model, and see the parameters in each layer. Note that the activation and MaxPooling layers do not have any learnable parameters. Also note the sizes of parameters for each layer.\n",
        "\n",
        "- For convolutional layer weights, it is output_channels x input_channels x window_width x window_height (4D tensor).\n",
        "- For convolutional layer biases, it is output_channels.\n",
        "- For linear layer weights, it is output_size x input_size\n",
        "- For linear layer biases, it is output_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uPkNRmsC25R"
      },
      "source": [
        "lenet = LeNet()\n",
        "lenet = lenet.to(device)  # Making the lenet to run on available runtime\n",
        "\n",
        "# Print out the size of parameters of each layer\n",
        "# state_dict() is simply a Python dictionary object that maps each layer to its parameter tensor\n",
        "for name, param in lenet.state_dict().items():\n",
        "    print(name, '\\n', param.size(), '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4DZeOyVfNzp"
      },
      "source": [
        "print(lenet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8ZaomZlC25e"
      },
      "source": [
        "### Defining Loss Function and Optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mzSRNhyxt_L"
      },
      "source": [
        "The **loss function** is a way of measuring the difference between the current prediction of the network and the correct prediction. As we saw in the lecture, the gradient descent algorithm is essentially adjusting the learnable parameters (weights and biases) of the network so as to decrease the loss. Here we will be using the **cross entropy loss**, which is commonly used for classification tasks (predicting a class from 0 to 9).\n",
        "\n",
        "The **learning rate** is a small fraction which is used to multiply the gradients of the loss function with respect to the weights. The idea behind doing this is that, we do not want to make drastic change to the weights of the neural network in each step, but rather a gradual one. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQRWcL7fC25f"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set the learning rate\n",
        "# Try with different learning rates\n",
        "learning_rate = 0.001\n",
        "\n",
        "optimizer = torch.optim.Adam(lenet.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5aRiuEt_Ykw"
      },
      "source": [
        "def count_parameters(model):\n",
        "    # numel() returns the total number of elements in the input tensor\n",
        "    return sum(param.numel() for param in lenet.parameters() if param.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(lenet):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmNz24u_n5fw"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "In Training Phase, we iterate over a batch of images in the train_loader. For each batch, we perform  the following steps:\n",
        "\n",
        "* First we zero out the gradients using zero_grad()\n",
        "\n",
        "* We pass the data to the model i.e. we perform forward pass by calling the forward()\n",
        "\n",
        "* We calculate the loss using the actual and predicted labels\n",
        "\n",
        "* Perform Backward pass using backward() to update the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNjXeIwM0qBv"
      },
      "source": [
        "# No of Epochs\n",
        "epoch = 10\n",
        "\n",
        "# First switch the module mode to lenet.train() so that new weights can be learned after every epoch. \n",
        "lenet.train()\n",
        "train_losses, train_accuracy = [], []\n",
        "\n",
        "# Loop through no of epochs\n",
        "for e in range(epoch):\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    # Iterate through all the batches in each epoch\n",
        "    for images, labels in train_loader:\n",
        "\n",
        "      # Convert the image and label to gpu for faster execution\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # Zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Passing the data to the model (Forward Pass)\n",
        "      predictions = lenet(images)\n",
        "\n",
        "      # Calculating the loss\n",
        "      loss = criterion(predictions, labels)\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # Performing backward pass (Backpropagation)\n",
        "      loss.backward()\n",
        "\n",
        "      # optimizer.step() updates the weights accordingly\n",
        "      optimizer.step()\n",
        "\n",
        "      # Accuracy calculation\n",
        "      _, predicted = torch.max(predictions, 1)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_losses.append(train_loss/len(mnist_train))\n",
        "    train_accuracy.append(100 * correct/len(mnist_train))\n",
        "    print('Epoch: {}, Train Loss:{:.6f}, Train Accuracy: {:.2f} '.format(e+1, train_losses[-1], train_accuracy[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sUUA_LU7YAp"
      },
      "source": [
        "### Visualization of Training Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlvBzC58fmJP"
      },
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.arange(1,epoch+1), train_losses)\n",
        "plt.title('Training Loss vs No. of epochs')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2fo6SPxC25r"
      },
      "source": [
        "### Evaluating the model\n",
        "\n",
        "In Testing Phase, we iterate over a batch of images in the test_loader. For each batch we perform the following steps:\n",
        "\n",
        "* We pass the images through the model (network) to get the outputs\n",
        "* Pick the class / label with the highest probability\n",
        "* Calculate the accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKk8LeBdC25t"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# switch the module mode to lenet.eval() for testing\n",
        "lenet.eval()\n",
        "\n",
        "for images, labels in test_loader:\n",
        "    \n",
        "    # Convert images and labels to gpu runtime for faster execution\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "        \n",
        "    # Passing the data to the model (Forward Pass)\n",
        "    result = lenet(images)\n",
        " \n",
        "    # Finding the prediction with the max probability\n",
        "    _,pred = torch.max(result, 1)\n",
        "    total += labels.size(0)\n",
        "    \n",
        "    # correct is incremented by the numer of prediction which are correct (equal to the ground truth labels)\n",
        "    correct += (pred == labels).sum().item()\n",
        "    \n",
        "print(\"Accuracy of Test Data: {0:.2f}%\".format(correct/total *100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2EkA9MVDQPE"
      },
      "source": [
        "As convolutional layers learn their own values for the filters we can do the same thing and see how our filters process images.\n",
        "\n",
        "Let's define a function that takes a batch of images and multiple filters which it then uses on the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VghgrACmCVCJ"
      },
      "source": [
        "def plot_filtered_images(images, filters):\n",
        "\n",
        "    images = torch.cat([i.unsqueeze(0) for i in images], dim = 0)\n",
        "    filters = filters.cpu()\n",
        "\n",
        "    n_images = images.shape[0]\n",
        "    n_filters = filters.shape[0]\n",
        "\n",
        "    filtered_images = F.conv2d(images, filters)\n",
        "\n",
        "    fig = plt.figure(figsize = (20, 10))\n",
        "\n",
        "    for i in range(n_images):\n",
        "        # Original Image\n",
        "        ax = fig.add_subplot(n_images, n_filters+1, i+1+(i*n_filters))\n",
        "        ax.imshow(images[i].squeeze(0), cmap = 'gray')\n",
        "        ax.set_title('Original')\n",
        "        ax.axis('off')\n",
        "\n",
        "        for j in range(n_filters):\n",
        "            # Filtered Images\n",
        "            image = filtered_images[i][j]\n",
        "            ax = fig.add_subplot(n_images, n_filters+1, i+1+(i*n_filters)+j+1)\n",
        "            ax.imshow(image.numpy(), cmap = 'gray')\n",
        "            ax.set_title(f'Filter {j+1}')\n",
        "            ax.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYl42_qqe2oI"
      },
      "source": [
        "Some of our filters look for areas of black pixels, which has the effect of inverting our images. Some apply a blur effect which is similar to Gaussian blur effects used in image processing to reduce noise in the image. Some detect edges like our Sobel filters we created earlier.\n",
        "\n",
        "There is no guarantee each filter will learn a unique filtering effect. They are initialized randomly and some of the filters may learn very similar weights. Therefore, it does not necessarily mean that using more filters will give a better performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvVmsnMVDe6_"
      },
      "source": [
        "N_IMAGES = 5\n",
        "\n",
        "images = [image for image, label in [mnist_test[i] for i in range(N_IMAGES)]]\n",
        "\n",
        "# Visualization of weights of first convolutional layer from the architecture\n",
        "filters = lenet.conv1.weight.data\n",
        "\n",
        "plot_filtered_images(images, filters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hEk1mkcR6e2"
      },
      "source": [
        "# Visualization of original filters\n",
        "def plot_filters(filters):\n",
        "\n",
        "    filters = filters.cpu()\n",
        "\n",
        "    n_filters = filters.shape[0]\n",
        "\n",
        "    fig = plt.figure(figsize = (20, 10))\n",
        "\n",
        "    for i in range(n_filters):\n",
        "        ax = fig.add_subplot(1, n_filters, i+1)\n",
        "        ax.imshow(filters[i].squeeze(0), cmap = 'gray')\n",
        "        ax.set_title(f'Filter {i+1}')\n",
        "        ax.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIHbi0C4Dizs"
      },
      "source": [
        "plot_filters(filters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97_jAo5AYby7"
      },
      "source": [
        "##  AlexNet (Pre-trained Model)\n",
        "\n",
        "\n",
        "The AlexNet CNN architecture won the 2012 ImageNet ILSVRC challenge by a\n",
        "large margin. It was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. It was the first to stack convolutional layers directly on top of one another, instead of stacking a pooling layer on top of each convolutional layer.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/AlexNet.png\" width=700px/>\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkbeVuycN4nH"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "AlexNet was the first convolutional network built for image classification\n",
        "\n",
        "1.      AlexNet architecture consists of 5 convolutional layers, 3 max-pooling layers, 2 normalization layers, 2 fully connected layers, and 1 softmax layer. \n",
        "\n",
        "2.      Each convolutional layer consists of convolutional filters and a nonlinear activation function ReLU. \n",
        "\n",
        "3.      The pooling layers are used to perform max pooling. \n",
        "\n",
        "4.      Input size is fixed due to the presence of fully connected layers.\n",
        "\n",
        "5.      The input size is mentioned at most of the places as 1x224x224 but due to some padding which happens it works out to be 1x227x227 \n",
        "\n",
        "6.      AlexNet overall has 60 million parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvyp4rH0DkBG"
      },
      "source": [
        "# Resize the data to pass through the Alexnet model\n",
        "# Normalize with mean and std (0.1307 and 0.3081 are the mean and std of MNIST data)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13BVeCoQDkBJ"
      },
      "source": [
        "# Loading the train set file\n",
        "trainset = datasets.MNIST(root='../data', \n",
        "                            train=True, \n",
        "                            transform=transform,  \n",
        "                            download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvXUBTm_DkBK"
      },
      "source": [
        "# Loading the test set file\n",
        "testset = datasets.MNIST(root='../data', \n",
        "                           train=False, \n",
        "                           transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt7XsPKkDkBP"
      },
      "source": [
        "# The mini batch size used for training\n",
        "batch_size = 128\n",
        "\n",
        "# Loading the train dataset\n",
        "# Data Loader loads the images and corresponding labels of defined mini batch size.\n",
        "# the image batch shape will be (batch_size, 1, 224, 224)\n",
        "trainloader = torch.utils.data.DataLoader(dataset=trainset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "# Loading the test dataset\n",
        "# Data loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.\n",
        "testloader = torch.utils.data.DataLoader(dataset=testset, \n",
        "                                          batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWYm5tj4qvxs"
      },
      "source": [
        "\n",
        "The first element of training data (X_train) is a 4th-order tensor of size (batch_size, 1, 224, 224), i.e. it consists of a batch of images of size 1x224x224 pixels where '1' represents one input image channel i.e. grey scale. y_train is a vector containing the correct classes (\"0\", \"1\", ..., \"9\") for each training digit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dTMcwTlRppw"
      },
      "source": [
        "for (X_train, y_train) in trainloader:\n",
        "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
        "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slm0zehlTFYP"
      },
      "source": [
        "### VIsulaizing the train images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4oX9-ZtQM9O"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=5,figsize=(15,5))          \n",
        "ax = axes.ravel()\n",
        "for i in range(10):\n",
        "    # Reshape the images to (224x224) for visualization\n",
        "    ax[i].imshow(X_train[i].reshape(224,224), cmap=\"gray\")\n",
        "    ax[i].title.set_text('Class: ' + str(y_train[i]))              \n",
        "plt.subplots_adjust(hspace=0.5)                                    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEQYr57fKTwN"
      },
      "source": [
        "### Loading the pre-trained AlexNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyQ4fchd1Yy8"
      },
      "source": [
        "model_alexnet = models.alexnet(pretrained = True) # Architecture and weights are downloaded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzWu4vy_1a-G"
      },
      "source": [
        "print(model_alexnet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRIxYTbw_mMb"
      },
      "source": [
        "# Change the input channels of the first layer to gray scale image 1x28x28\n",
        "model_alexnet.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "\n",
        "# Change the last layer output dimensions to 10 classes for mnist dataset\n",
        "model_alexnet.classifier[6] = nn.Linear(4096, 10)\n",
        "print(model_alexnet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSvM2AP01eMI"
      },
      "source": [
        "# Convert the model to CUDA\n",
        "model_alexnet.to(device)\n",
        "\n",
        "# Print the summary\n",
        "from torchsummary import summary\n",
        "summary(model_alexnet, (1, 224,224))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXS7bIQsLXB_"
      },
      "source": [
        "### Define the loss and optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FSGeeM2R6zw"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_alexnet = torch.optim.Adam(model_alexnet.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZBr6vHNVvDt"
      },
      "source": [
        "### Train the model using AlexNet\n",
        "\n",
        "The below code cell takes some time to complete the execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-Z-nFCP3u1N"
      },
      "source": [
        "# No of Epochs\n",
        "epoch = 5\n",
        "\n",
        "# First switch the module mode to lenet.train() so that new weights can be learned after every epoch. \n",
        "model_alexnet.train()\n",
        "train_losses, train_accuracy = [], []\n",
        "\n",
        "# Loop through no of epochs\n",
        "for e in range(epoch):\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    # Iterate through all the batches in each epoch\n",
        "    for images, labels in trainloader:\n",
        "\n",
        "      # Convert the image and label to gpu for faster execution\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # Zero the parameter gradients\n",
        "      optimizer_alexnet.zero_grad()\n",
        "\n",
        "      # Passing the data to the model (Forward Pass)\n",
        "      predictions = model_alexnet(images)\n",
        "\n",
        "      # Calculating the loss\n",
        "      loss = criterion(predictions, labels)\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # Performing backward pass (Backpropagation)\n",
        "      loss.backward()\n",
        "\n",
        "      # optimizer.step() updates the weights accordingly\n",
        "      optimizer_alexnet.step()\n",
        "\n",
        "      # Accuracy calculation\n",
        "      _, predicted = torch.max(predictions, 1)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_losses.append(train_loss/len(trainset))\n",
        "    train_accuracy.append(100 * correct/len(trainset))\n",
        "    print('Epoch: {}, Train Loss:{:.6f}, Train Accuracy: {:.2f} '.format(e+1, train_losses[-1], train_accuracy[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-HlYf6rOK_P"
      },
      "source": [
        "### Visualization of Training Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr4dCIxcOK_P"
      },
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.arange(1, epoch+1), train_losses)\n",
        "plt.title('Training Loss vs No. of epochs')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kc0hoVuLL84"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-yKAIxx5uR2"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Switch the model to eval() for testing\n",
        "model_alexnet.eval()\n",
        "\n",
        "for images, labels in testloader:\n",
        "    \n",
        "    # Convert images and labels to gpu runtime for faster execution\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "        \n",
        "    # Passing the data to the model (Forward Pass)\n",
        "    result = model_alexnet(images)\n",
        " \n",
        "    # Finding the prediction with the max probability\n",
        "    _,pred = torch.max(result, 1)\n",
        "    total += labels.size(0)\n",
        "    \n",
        "    # correct is incremented by the numer of prediction which are correct (equal to the ground truth labels)\n",
        "    correct += (pred == labels).sum().item()\n",
        "    \n",
        "print(\"Accuracy of Test Data: {0:.2f}%\".format(correct/total *100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRwvwQ8_VbWR"
      },
      "source": [
        "#### Consider the following statements and answer Q1.\n",
        "\n",
        "A. Zero padding is a technique that can allow us to keep the spatial size of the input and output of a convolutional layer to be the same.\n",
        "\n",
        "B. Convolution is an extremely efficient way of describing transformations that apply the same linear transformation of a small local region across the entire input.\n",
        "\n",
        "C. Max-pooling helps pick important features at a reduced spatial resolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlhryqWxbXMz",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. Which of the above statement(s) is/are True?\n",
        "Answer1 = \"Only A and C\" #@param [\"\",\"Only A and C\",\"Only B and C\", \"Only B\", \"Only A and B\", \"A, B and C\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8R6F2KGyiwk",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. If max pooling is performed on a feature map of 256x256, with a kernel size of 2x2 and stride 2x2, what is the dimension of the resulting output?\n",
        "Answer2 = \"64x64\" #@param [\"\",\"64x64\", \"32x32\",\"128x128\",\"256x256\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"NA\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}